{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "590f4e95-e064-4e9a-99b5-e576d88953de",
      "metadata": {
        "tags": [],
        "id": "590f4e95-e064-4e9a-99b5-e576d88953de"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.functional import F\n",
        "from tqdm import trange\n",
        "import numpy as np\n",
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrxUclDI44gI",
        "outputId": "910cfd54-090a-4385-fe81-e09a7ca253ef"
      },
      "id": "jrxUclDI44gI",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/phase4 public'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnvfBIus5Hww",
        "outputId": "88e0fb34-a3e5-40ce-8c1e-f2d4e1f2a6a6"
      },
      "id": "FnvfBIus5Hww",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1olR9djsPqRY8dxn01k2qeMnqfOfqWRQE/phase4 public\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZaJuwrUu6Xx",
        "outputId": "aea2e0d5-ca26-472b-e7e2-4d5b45d91976"
      },
      "id": "dZaJuwrUu6Xx",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'CI - Spring_2023 - phase4.pdf'   exploring_data.py   utils.py\n",
            " data\t\t\t\t  __pycache__\n",
            " dataloader_demo.ipynb\t\t  triplet_loss.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import get_data_loaders\n",
        "import torch\n",
        "\n",
        "def loaders_demo():\n",
        "    full_dataloaders, _ = get_data_loaders(\n",
        "        filenames={\n",
        "            'train': './data/12000_train_mnistmnistmsvhnsynusps.npz',\n",
        "            'test': './data/12000_test_mnistmnistmsvhnsynusps.npz',\n",
        "        },\n",
        "        batch_size= 128\n",
        "    )\n",
        "    return full_dataloaders\n",
        "\n",
        "full_dataloaders = loaders_demo()\n",
        "print(full_dataloaders.keys())\n",
        "\n",
        "for phase in ['train', 'test', 'test_missing']:\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(full_dataloaders[phase]):\n",
        "        print(f'{batch_indx}-th batch')\n",
        "        print('images shape: ', images.shape)\n",
        "        print('features shape: ', features.shape)\n",
        "        if phase == 'test_missing':\n",
        "            print('in test-missing dataloaders, since the features are not available, features are filled with zeros', torch.sum(features))\n",
        "        print('domain labels freq: ', torch.unique(domain_labels, return_counts=True))\n",
        "        print('digit labels freq: ', torch.unique(digit_labels, return_counts=True))\n",
        "        print()\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acv3G2Er5LAG",
        "outputId": "9f53ed97-5861-4021-94da-1a2020c40744"
      },
      "id": "Acv3G2Er5LAG",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datafiles to read:  {'train': './data/12000_train_mnistmnistmsvhnsynusps.npz', 'test': './data/12000_test_mnistmnistmsvhnsynusps.npz'}\n",
            "reading ./data/12000_train_mnistmnistmsvhnsynusps.npz, number of samples: 60000\n",
            "reading ./data/12000_test_mnistmnistmsvhnsynusps.npz, number of samples: 21600\n",
            "reading ./data/12000_test_mnistmnistmsvhnsynusps.npz, number of samples: 21600\n",
            "dict_keys(['train', 'test', 'test_missing', 'train_size', 'test_size', 'test_missing_size'])\n",
            "0-th batch\n",
            "images shape:  torch.Size([128, 3, 32, 32])\n",
            "features shape:  torch.Size([128, 256])\n",
            "domain labels freq:  (tensor([0, 1, 2, 3, 4]), tensor([22, 25, 20, 35, 26]))\n",
            "digit labels freq:  (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([11, 14, 14, 14, 13, 17, 13, 11, 13,  8]))\n",
            "\n",
            "0-th batch\n",
            "images shape:  torch.Size([128, 3, 32, 32])\n",
            "features shape:  torch.Size([128, 256])\n",
            "domain labels freq:  (tensor([0, 1, 2, 3, 4]), tensor([22, 18, 26, 31, 31]))\n",
            "digit labels freq:  (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([10, 20, 13, 10,  9, 15, 11, 10, 15, 15]))\n",
            "\n",
            "0-th batch\n",
            "images shape:  torch.Size([128, 3, 32, 32])\n",
            "features shape:  torch.Size([128, 256])\n",
            "in test-missing dataloaders, since the features are not available, features are filled with zeros tensor(0.)\n",
            "domain labels freq:  (tensor([0, 1, 2, 3, 4]), tensor([35, 18, 26, 23, 26]))\n",
            "digit labels freq:  (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([13, 19, 17,  9, 14, 17, 12,  9, 10,  8]))\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8c4e4b9a-b271-4258-9205-657ffbf511fb",
      "metadata": {
        "id": "8c4e4b9a-b271-4258-9205-657ffbf511fb"
      },
      "outputs": [],
      "source": [
        "torch.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv5HautEdocj",
        "outputId": "300f1b11-bdd1-47b6-eced-858e868bcde7"
      },
      "id": "sv5HautEdocj",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "08975e50-8276-48d0-bfe5-38164ec80855",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08975e50-8276-48d0-bfe5-38164ec80855",
        "outputId": "c19e5679-2e79-4b91-f7a7-391e1a26e9f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 21600)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(full_dataloaders['train'].dataset), len(full_dataloaders['test'].dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "058e11e3-1407-4deb-bc24-e74bf63bb639",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "058e11e3-1407-4deb-bc24-e74bf63bb639",
        "outputId": "77453799-e53e-4af9-b471-f8841a301b79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     6.037965 -1.799968  1.182611 -0.248518 -0.320674  0.251610  0.734077   \n",
              "1   -12.564800  0.517854  1.038122 -0.489715 -1.271697 -0.718387  2.967587   \n",
              "2     7.445361 -0.557834 -1.135024  0.368366  0.614173  0.158659 -0.522470   \n",
              "3     0.744837 -4.175048  0.368809  0.091318  1.096337  0.079871  0.053634   \n",
              "4     0.228644 -3.329779 -2.187275  3.250448  4.089767 -0.171248 -0.909210   \n",
              "..         ...       ...       ...       ...       ...       ...       ...   \n",
              "123 -14.531523 -6.808526 -1.962304 -2.109931  2.470517  0.892024  1.383679   \n",
              "124   8.207400  5.156352 -3.013297  0.255962  2.002417 -2.155067  4.849430   \n",
              "125 -11.028695  2.798279 -1.152583 -0.779837  3.369474 -0.146005 -0.842274   \n",
              "126 -10.087826  4.747341  0.836692 -5.564118 -1.667153 -1.559360 -3.061188   \n",
              "127 -12.342452 -0.540276 -0.401978  0.312520 -4.852377 -1.126977 -0.722765   \n",
              "\n",
              "          7         8         9    ...       246       247       248  \\\n",
              "0    0.608663  0.212084  0.767771  ... -0.072638  0.316827  0.023135   \n",
              "1    3.289755  0.988581 -0.101411  ... -0.454926 -0.120079 -0.101010   \n",
              "2   -0.589508  0.290434 -0.124693  ... -0.135883 -0.080708  0.098077   \n",
              "3   -0.415834 -0.559677  0.074155  ... -0.032611 -0.089508 -0.032700   \n",
              "4   -0.898119 -4.993124 -0.198844  ...  0.128845  0.038883 -0.115404   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "123 -2.025483 -0.928582  0.343404  ...  0.074560  0.105696 -0.235819   \n",
              "124  0.571528  3.321007 -1.156567  ...  0.072982 -0.017911  0.056130   \n",
              "125 -4.763255  6.170467 -2.651726  ... -0.179539  0.107593 -0.226426   \n",
              "126 -0.031718  2.186175  1.350780  ...  0.024626 -0.005803 -0.003717   \n",
              "127  0.111993 -1.917904 -0.856894  ...  0.012340 -0.019494  0.009521   \n",
              "\n",
              "          249       250       251       252       253       254       255  \n",
              "0   -0.329847 -0.159941 -0.212282  0.070331 -0.110758 -0.065389 -0.018419  \n",
              "1   -0.171803 -0.297529  0.160597 -0.018645 -0.045893 -0.045439 -0.096207  \n",
              "2   -0.099581  0.086981 -0.015576  0.033817 -0.171754 -0.028527 -0.012392  \n",
              "3   -0.127731 -0.191442 -0.135071 -0.055170  0.002007 -0.098564  0.124951  \n",
              "4    0.339894 -0.298737 -0.154749 -0.035553  0.143919  0.050182  0.179309  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "123 -0.009158  0.068220  0.018031  0.079444 -0.026290  0.098710 -0.030104  \n",
              "124 -0.089698 -0.018625  0.021709 -0.126845  0.075862  0.179969  0.049354  \n",
              "125  0.093342 -0.087881  0.130228  0.142368  0.333962  0.225531  0.132795  \n",
              "126 -0.054994 -0.009849  0.031435 -0.002613  0.011187 -0.032867  0.009332  \n",
              "127 -0.004061  0.043480  0.011145 -0.016540  0.011772  0.004417 -0.016897  \n",
              "\n",
              "[128 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e1775cf-b8a0-40cd-baca-7604ca275b33\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.037965</td>\n",
              "      <td>-1.799968</td>\n",
              "      <td>1.182611</td>\n",
              "      <td>-0.248518</td>\n",
              "      <td>-0.320674</td>\n",
              "      <td>0.251610</td>\n",
              "      <td>0.734077</td>\n",
              "      <td>0.608663</td>\n",
              "      <td>0.212084</td>\n",
              "      <td>0.767771</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072638</td>\n",
              "      <td>0.316827</td>\n",
              "      <td>0.023135</td>\n",
              "      <td>-0.329847</td>\n",
              "      <td>-0.159941</td>\n",
              "      <td>-0.212282</td>\n",
              "      <td>0.070331</td>\n",
              "      <td>-0.110758</td>\n",
              "      <td>-0.065389</td>\n",
              "      <td>-0.018419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-12.564800</td>\n",
              "      <td>0.517854</td>\n",
              "      <td>1.038122</td>\n",
              "      <td>-0.489715</td>\n",
              "      <td>-1.271697</td>\n",
              "      <td>-0.718387</td>\n",
              "      <td>2.967587</td>\n",
              "      <td>3.289755</td>\n",
              "      <td>0.988581</td>\n",
              "      <td>-0.101411</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.454926</td>\n",
              "      <td>-0.120079</td>\n",
              "      <td>-0.101010</td>\n",
              "      <td>-0.171803</td>\n",
              "      <td>-0.297529</td>\n",
              "      <td>0.160597</td>\n",
              "      <td>-0.018645</td>\n",
              "      <td>-0.045893</td>\n",
              "      <td>-0.045439</td>\n",
              "      <td>-0.096207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.445361</td>\n",
              "      <td>-0.557834</td>\n",
              "      <td>-1.135024</td>\n",
              "      <td>0.368366</td>\n",
              "      <td>0.614173</td>\n",
              "      <td>0.158659</td>\n",
              "      <td>-0.522470</td>\n",
              "      <td>-0.589508</td>\n",
              "      <td>0.290434</td>\n",
              "      <td>-0.124693</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.135883</td>\n",
              "      <td>-0.080708</td>\n",
              "      <td>0.098077</td>\n",
              "      <td>-0.099581</td>\n",
              "      <td>0.086981</td>\n",
              "      <td>-0.015576</td>\n",
              "      <td>0.033817</td>\n",
              "      <td>-0.171754</td>\n",
              "      <td>-0.028527</td>\n",
              "      <td>-0.012392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.744837</td>\n",
              "      <td>-4.175048</td>\n",
              "      <td>0.368809</td>\n",
              "      <td>0.091318</td>\n",
              "      <td>1.096337</td>\n",
              "      <td>0.079871</td>\n",
              "      <td>0.053634</td>\n",
              "      <td>-0.415834</td>\n",
              "      <td>-0.559677</td>\n",
              "      <td>0.074155</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.032611</td>\n",
              "      <td>-0.089508</td>\n",
              "      <td>-0.032700</td>\n",
              "      <td>-0.127731</td>\n",
              "      <td>-0.191442</td>\n",
              "      <td>-0.135071</td>\n",
              "      <td>-0.055170</td>\n",
              "      <td>0.002007</td>\n",
              "      <td>-0.098564</td>\n",
              "      <td>0.124951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.228644</td>\n",
              "      <td>-3.329779</td>\n",
              "      <td>-2.187275</td>\n",
              "      <td>3.250448</td>\n",
              "      <td>4.089767</td>\n",
              "      <td>-0.171248</td>\n",
              "      <td>-0.909210</td>\n",
              "      <td>-0.898119</td>\n",
              "      <td>-4.993124</td>\n",
              "      <td>-0.198844</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128845</td>\n",
              "      <td>0.038883</td>\n",
              "      <td>-0.115404</td>\n",
              "      <td>0.339894</td>\n",
              "      <td>-0.298737</td>\n",
              "      <td>-0.154749</td>\n",
              "      <td>-0.035553</td>\n",
              "      <td>0.143919</td>\n",
              "      <td>0.050182</td>\n",
              "      <td>0.179309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>-14.531523</td>\n",
              "      <td>-6.808526</td>\n",
              "      <td>-1.962304</td>\n",
              "      <td>-2.109931</td>\n",
              "      <td>2.470517</td>\n",
              "      <td>0.892024</td>\n",
              "      <td>1.383679</td>\n",
              "      <td>-2.025483</td>\n",
              "      <td>-0.928582</td>\n",
              "      <td>0.343404</td>\n",
              "      <td>...</td>\n",
              "      <td>0.074560</td>\n",
              "      <td>0.105696</td>\n",
              "      <td>-0.235819</td>\n",
              "      <td>-0.009158</td>\n",
              "      <td>0.068220</td>\n",
              "      <td>0.018031</td>\n",
              "      <td>0.079444</td>\n",
              "      <td>-0.026290</td>\n",
              "      <td>0.098710</td>\n",
              "      <td>-0.030104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>8.207400</td>\n",
              "      <td>5.156352</td>\n",
              "      <td>-3.013297</td>\n",
              "      <td>0.255962</td>\n",
              "      <td>2.002417</td>\n",
              "      <td>-2.155067</td>\n",
              "      <td>4.849430</td>\n",
              "      <td>0.571528</td>\n",
              "      <td>3.321007</td>\n",
              "      <td>-1.156567</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072982</td>\n",
              "      <td>-0.017911</td>\n",
              "      <td>0.056130</td>\n",
              "      <td>-0.089698</td>\n",
              "      <td>-0.018625</td>\n",
              "      <td>0.021709</td>\n",
              "      <td>-0.126845</td>\n",
              "      <td>0.075862</td>\n",
              "      <td>0.179969</td>\n",
              "      <td>0.049354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>-11.028695</td>\n",
              "      <td>2.798279</td>\n",
              "      <td>-1.152583</td>\n",
              "      <td>-0.779837</td>\n",
              "      <td>3.369474</td>\n",
              "      <td>-0.146005</td>\n",
              "      <td>-0.842274</td>\n",
              "      <td>-4.763255</td>\n",
              "      <td>6.170467</td>\n",
              "      <td>-2.651726</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.179539</td>\n",
              "      <td>0.107593</td>\n",
              "      <td>-0.226426</td>\n",
              "      <td>0.093342</td>\n",
              "      <td>-0.087881</td>\n",
              "      <td>0.130228</td>\n",
              "      <td>0.142368</td>\n",
              "      <td>0.333962</td>\n",
              "      <td>0.225531</td>\n",
              "      <td>0.132795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>-10.087826</td>\n",
              "      <td>4.747341</td>\n",
              "      <td>0.836692</td>\n",
              "      <td>-5.564118</td>\n",
              "      <td>-1.667153</td>\n",
              "      <td>-1.559360</td>\n",
              "      <td>-3.061188</td>\n",
              "      <td>-0.031718</td>\n",
              "      <td>2.186175</td>\n",
              "      <td>1.350780</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024626</td>\n",
              "      <td>-0.005803</td>\n",
              "      <td>-0.003717</td>\n",
              "      <td>-0.054994</td>\n",
              "      <td>-0.009849</td>\n",
              "      <td>0.031435</td>\n",
              "      <td>-0.002613</td>\n",
              "      <td>0.011187</td>\n",
              "      <td>-0.032867</td>\n",
              "      <td>0.009332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>-12.342452</td>\n",
              "      <td>-0.540276</td>\n",
              "      <td>-0.401978</td>\n",
              "      <td>0.312520</td>\n",
              "      <td>-4.852377</td>\n",
              "      <td>-1.126977</td>\n",
              "      <td>-0.722765</td>\n",
              "      <td>0.111993</td>\n",
              "      <td>-1.917904</td>\n",
              "      <td>-0.856894</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012340</td>\n",
              "      <td>-0.019494</td>\n",
              "      <td>0.009521</td>\n",
              "      <td>-0.004061</td>\n",
              "      <td>0.043480</td>\n",
              "      <td>0.011145</td>\n",
              "      <td>-0.016540</td>\n",
              "      <td>0.011772</td>\n",
              "      <td>0.004417</td>\n",
              "      <td>-0.016897</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>128 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e1775cf-b8a0-40cd-baca-7604ca275b33')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e1775cf-b8a0-40cd-baca-7604ca275b33 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e1775cf-b8a0-40cd-baca-7604ca275b33');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(full_dataloaders['train']):\n",
        "    if batch_indx < 1:\n",
        "        break\n",
        "import pandas as pd\n",
        "pd.DataFrame(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c4a158c4-ece8-4c48-a279-b989b799fbef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4a158c4-ece8-4c48-a279-b989b799fbef",
        "outputId": "6ca945b3-9e7c-4c22-8653-1b23a6551942"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 469)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "batch_indx, len(full_dataloaders['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10895013-9a4a-4d8b-95c8-905022821dbc",
      "metadata": {
        "id": "10895013-9a4a-4d8b-95c8-905022821dbc"
      },
      "source": [
        "### Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f2b329db-4b19-4472-946e-6ee4e19f9b2f",
      "metadata": {
        "id": "f2b329db-4b19-4472-946e-6ee4e19f9b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26b7eb0-6f92-4559-d866-555e97e8a610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demo_model(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=3456, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class demo_model(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # kernel\n",
        "        self.conv1 = torch.nn.Conv2d(3, 6, 5, stride=1, padding=0)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 3, stride=1, padding=0)\n",
        "        self.conv3 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=0)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(in_features=3200 + 256, out_features=1024)\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=128)\n",
        "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "    # Forward Pass\n",
        "    def forward(self, inputs, inputs2, debug=False):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(inputs)), 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 1)\n",
        "        x = F.max_pool2d(F.relu(self.conv3(x)), 1)\n",
        "        x0 = self.flatten(x)\n",
        "\n",
        "        use_gpu = True\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
        "        merged_input = torch.zeros(128, 3456)\n",
        "        merged_input = merged_input.to(device)\n",
        "\n",
        "        for i in range(x0.shape[0]):\n",
        "            merged_input[i] = torch.cat((x0[i], inputs2[i]))\n",
        "\n",
        "\n",
        "        x1 = self.fc1(merged_input)\n",
        "        x1 = F.relu(x1)\n",
        "        x2 = self.fc2(x1)\n",
        "        x2 = F.relu(x2)\n",
        "        outputs = self.fc3(x2)\n",
        "\n",
        "        if debug:\n",
        "            print('inputs shape: ', inputs.shape) # inputs in shape [N, C, H, W]\n",
        "            print('after flattening: ', x0.shape)\n",
        "            print('Activations after 1st fully connected layer: ', x1.shape)\n",
        "            print('Activations after 2nd fully connected layer: ', x2.shape)\n",
        "            print('Output shape: ', outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "model = demo_model()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0f26ebd8-98a0-4875-aec5-e54d103cfc13",
      "metadata": {
        "id": "0f26ebd8-98a0-4875-aec5-e54d103cfc13"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, optim: torch.optim.Optimizer,\n",
        "         dataloader: DataLoader, loss_fn):\n",
        "\n",
        "    # utils\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train() #\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
        "        if digit_labels.shape[0] != 128:\n",
        "            continue\n",
        "        images = images.to(device)\n",
        "        features = features.to(device)\n",
        "        digit_labels = digit_labels.to(device)\n",
        "\n",
        "        outputs = model(images, features) # Forward Pass, [N, 10]\n",
        "        loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
        "\n",
        "        loss.backward() # Compute Gradients\n",
        "        optim.step() # Update parameters\n",
        "        optim.zero_grad() # zero the parameter's gradients\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1) # Explain, [N]\n",
        "        running_corrects += torch.sum(preds == digit_labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "\n",
        "\n",
        "    return epoch_acc, epoch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1042f4bc-af39-4fe9-9ac5-04a03e9a7d5d",
      "metadata": {
        "id": "1042f4bc-af39-4fe9-9ac5-04a03e9a7d5d"
      },
      "source": [
        "## Evaluating Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "90956779-3d38-41ab-924e-e2c6ac6c6671",
      "metadata": {
        "id": "90956779-3d38-41ab-924e-e2c6ac6c6671"
      },
      "outputs": [],
      "source": [
        "def test_model(model: nn.Module,\n",
        "         dataloader: DataLoader, loss_fn):\n",
        "\n",
        "    # utils\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    with torch.no_grad():\n",
        "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
        "            if digit_labels.shape[0] != 128:\n",
        "                continue\n",
        "            images = images.to(device)\n",
        "            features = features.to(device)\n",
        "            digit_labels = digit_labels.to(device)\n",
        "\n",
        "            outputs = model(images, features) # Forward Pass\n",
        "            loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
        "\n",
        "            _, preds = torch.max(outputs, 1) #\n",
        "            running_corrects += torch.sum(preds == digit_labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    test_acc = (running_corrects / num_samples) * 100\n",
        "    test_loss = (running_loss / num_batches)\n",
        "\n",
        "    return test_acc, test_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def custom_plot_training_stats(acc_hist, loss_hist, phase_list, title: str, dir: str):\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=[14, 6], dpi=100)\n",
        "\n",
        "    for phase in phase_list:\n",
        "        lowest_loss_x = np.argmin(np.array(loss_hist[phase]))\n",
        "        lowest_loss_y = loss_hist[phase][lowest_loss_x]\n",
        "\n",
        "        ax1.annotate(\"{:.4f}\".format(lowest_loss_y), [lowest_loss_x, lowest_loss_y])\n",
        "        ax1.plot(loss_hist[phase], '-x', label=f'{phase} loss', markevery = [lowest_loss_x])\n",
        "\n",
        "        ax1.set_xlabel(xlabel='epochs')\n",
        "        ax1.set_ylabel(ylabel='loss')\n",
        "\n",
        "        ax1.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax1.legend()\n",
        "        ax1.label_outer()\n",
        "\n",
        "    # acc:\n",
        "    for phase in phase_list:\n",
        "        highest_acc_x = np.argmax(np.array(acc_hist[phase].cpu()))\n",
        "        highest_acc_y = acc_hist[phase][highest_acc_x]\n",
        "\n",
        "        ax2.annotate(\"{:.4f}\".format(highest_acc_y), [highest_acc_x, highest_acc_y])\n",
        "        ax2.plot(acc_hist[phase], '-x', label=f'{phase} loss', markevery = [highest_acc_x])\n",
        "\n",
        "        ax2.set_xlabel(xlabel='epochs')\n",
        "        ax2.set_ylabel(ylabel='acc')\n",
        "\n",
        "        ax2.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax2.legend()\n",
        "        #ax2.label_outer()\n",
        "\n",
        "    fig.suptitle(f'{title}')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fMbOQp5B5Ql1"
      },
      "id": "fMbOQp5B5Ql1",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4fd46bbd-532a-4f57-a027-34a762c191de",
      "metadata": {
        "id": "4fd46bbd-532a-4f57-a027-34a762c191de"
      },
      "outputs": [],
      "source": [
        "def demo():\n",
        "    batch_size = 128\n",
        "    num_epochs = 25\n",
        "    learning_rate = 0.005\n",
        "\n",
        "    model = demo_model()\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_history = {'train': [], 'test': []}\n",
        "    loss_history = {'train': [], 'test': []}\n",
        "\n",
        "    for epoch in trange(num_epochs):\n",
        "        train_acc, train_loss = train_one_epoch(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
        "        test_acc, test_loss = test_model(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
        "\n",
        "        acc_history['train'].append(train_acc)\n",
        "        acc_history['test'].append(test_acc)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['test'].append(test_loss)\n",
        "\n",
        "    # custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
        "    print(acc_history)\n",
        "    print(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f8a03b45-5d6f-4699-a308-f335c17d0a8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8a03b45-5d6f-4699-a308-f335c17d0a8f",
        "outputId": "4de90b4a-90f9-46ff-cbf6-f86f6083200f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [05:26<00:00, 13.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': [tensor(19.49, device='cuda:0'), tensor(27.70, device='cuda:0'), tensor(43.89, device='cuda:0'), tensor(52.16, device='cuda:0'), tensor(58.07, device='cuda:0'), tensor(63.15, device='cuda:0'), tensor(67.29, device='cuda:0'), tensor(70.59, device='cuda:0'), tensor(73.26, device='cuda:0'), tensor(75.71, device='cuda:0'), tensor(77.48, device='cuda:0'), tensor(79.17, device='cuda:0'), tensor(80.38, device='cuda:0'), tensor(81.45, device='cuda:0'), tensor(82.67, device='cuda:0'), tensor(83.64, device='cuda:0'), tensor(84.49, device='cuda:0'), tensor(85.12, device='cuda:0'), tensor(85.83, device='cuda:0'), tensor(86.60, device='cuda:0'), tensor(87.10, device='cuda:0'), tensor(87.78, device='cuda:0'), tensor(88.32, device='cuda:0'), tensor(88.87, device='cuda:0'), tensor(89.39, device='cuda:0')], 'test': [tensor(22.97, device='cuda:0'), tensor(34.48, device='cuda:0'), tensor(49.48, device='cuda:0'), tensor(56.41, device='cuda:0'), tensor(60.25, device='cuda:0'), tensor(64.32, device='cuda:0'), tensor(70.18, device='cuda:0'), tensor(73.09, device='cuda:0'), tensor(72.36, device='cuda:0'), tensor(76.92, device='cuda:0'), tensor(77.14, device='cuda:0'), tensor(78.58, device='cuda:0'), tensor(79.10, device='cuda:0'), tensor(80.44, device='cuda:0'), tensor(80.96, device='cuda:0'), tensor(81.55, device='cuda:0'), tensor(82.22, device='cuda:0'), tensor(82.61, device='cuda:0'), tensor(83.53, device='cuda:0'), tensor(84.37, device='cuda:0'), tensor(84.61, device='cuda:0'), tensor(85.07, device='cuda:0'), tensor(85.11, device='cuda:0'), tensor(85.60, device='cuda:0'), tensor(85.96, device='cuda:0')]}\n",
            "{'train': [2.2560561567481394, 2.0966874170405014, 1.7191804848245975, 1.4151564137513704, 1.2744898305518795, 1.1385629389331793, 1.019796398784052, 0.9205004685977375, 0.8389009408859301, 0.7705311377419591, 0.7172041814337408, 0.6702670792399693, 0.6299199596015629, 0.5952735616962539, 0.562129804828782, 0.5332002045630392, 0.5059982554109366, 0.4831794107646576, 0.46054822867358924, 0.4377467071196672, 0.41730132066746, 0.3982147440663787, 0.3784018243744429, 0.3605851503704657, 0.34318636500759164], 'test': [2.1895662341597517, 1.954062687574759, 1.4939354601696397, 1.313397962666122, 1.209712283851127, 1.0964874321892417, 0.9482363430706001, 0.8506272921900777, 0.8663963329862561, 0.7449965685076968, 0.729272447217851, 0.6894905449017971, 0.6677218178320213, 0.6304775903210837, 0.6140691452830501, 0.5949952838688912, 0.5758292724394939, 0.568636261499845, 0.5422319662288801, 0.5180872107398581, 0.5122824133147855, 0.5026335820291169, 0.4942165226978663, 0.48874364943193965, 0.4805361908921123]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Aff4PEsJBnh"
      },
      "id": "_Aff4PEsJBnh"
    },
    {
      "cell_type": "code",
      "source": [
        "class demo_model2(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # kernel\n",
        "        self.conv1 = torch.nn.Conv2d(3, 6, 5, stride=1, padding=0)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 3, stride=1, padding=0)\n",
        "        self.conv3 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=0)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(in_features=3200 + 256, out_features=1024)\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=128)\n",
        "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "    # Forward Pass\n",
        "    def forward(self, inputs, inputs2, debug=False):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(inputs)), 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 1)\n",
        "        x = F.max_pool2d(F.relu(self.conv3(x)), 1)\n",
        "        x0 = self.flatten(x)\n",
        "\n",
        "        use_gpu = True\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
        "        merged_input = torch.zeros(128, 3456)\n",
        "        merged_input = merged_input.to(device)\n",
        "\n",
        "        for i in range(x0.shape[0]):\n",
        "            merged_input[i] = torch.cat((x0[i], inputs2[i]))\n",
        "\n",
        "\n",
        "        x1 = self.fc1(merged_input)\n",
        "        x1 = F.relu(x1)\n",
        "        x2 = self.fc2(x1)\n",
        "        penultimate = F.relu(x2)\n",
        "        outputs = self.fc3(penultimate)\n",
        "\n",
        "        if debug:\n",
        "            print('inputs shape: ', inputs.shape) # inputs in shape [N, C, H, W]\n",
        "            print('after flattening: ', x0.shape)\n",
        "            print('Activations after 1st fully connected layer: ', x1.shape)\n",
        "            print('Activations after 2nd fully connected layer: ', x2.shape)\n",
        "            print('Output shape: ', outputs.shape)\n",
        "\n",
        "        return outputs, penultimate\n",
        "\n",
        "model = demo_model2()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF5_pzlLG9qB",
        "outputId": "ff9e0740-898f-4e46-b06e-0f0e2d9e27b4"
      },
      "id": "sF5_pzlLG9qB",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demo_model2(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=3456, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch2(model: nn.Module, optim: torch.optim.Optimizer,\n",
        "         dataloader: DataLoader, loss_fn, triplet_loss_fn):\n",
        "\n",
        "    # utils\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train() #\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
        "        if digit_labels.shape[0] != 128:\n",
        "            continue\n",
        "        # images = images.to(device)\n",
        "        # features = features.to(device)\n",
        "        # digit_labels = digit_labels.to(device)\n",
        "\n",
        "        outputs, penultimate = model(images, features) # Forward Pass, [N, 10]\n",
        "        loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
        "        loss2 = triplet_loss_fn(penultimate, domain_labels)\n",
        "        landa = 0.3\n",
        "        loss_total = loss + landa * loss2\n",
        "\n",
        "        loss_total.backward() # Compute Gradients\n",
        "        optim.step() # Update parameters\n",
        "        optim.zero_grad() # zero the parameter's gradients\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1) # Explain, [N]\n",
        "        running_corrects += torch.sum(preds == digit_labels)\n",
        "        running_loss += loss_total.item()\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "\n",
        "\n",
        "    return epoch_acc, epoch_loss"
      ],
      "metadata": {
        "id": "SriUjNHHEysw"
      },
      "id": "SriUjNHHEysw",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgSqvfXRJftZ"
      },
      "source": [
        "## Evaluating Model:"
      ],
      "id": "pgSqvfXRJftZ"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "LFDhQmnCJfta"
      },
      "outputs": [],
      "source": [
        "def test_model2(model: nn.Module,\n",
        "         dataloader: DataLoader, loss_fn, triplet_loss_fn):\n",
        "\n",
        "    # utils\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    with torch.no_grad():\n",
        "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
        "            if digit_labels.shape[0] != 128:\n",
        "                continue\n",
        "            # images = images.to(device)\n",
        "            # features = features.to(device)\n",
        "            # digit_labels = digit_labels.to(device)\n",
        "\n",
        "            outputs, penultimate = model(images, features) # Forward Pass\n",
        "            loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
        "            loss2 = triplet_loss_fn(penultimate, domain_labels)\n",
        "            landa = 2\n",
        "            loss_total = loss + landa * loss2\n",
        "\n",
        "            _, preds = torch.max(outputs, 1) #\n",
        "            running_corrects += torch.sum(preds == digit_labels)\n",
        "            running_loss += loss_total.item()\n",
        "\n",
        "    test_acc = (running_corrects / num_samples) * 100\n",
        "    test_loss = (running_loss / num_batches)\n",
        "\n",
        "    return test_acc, test_loss"
      ],
      "id": "LFDhQmnCJfta"
    },
    {
      "cell_type": "code",
      "source": [
        "def demo2():\n",
        "    batch_size = 128\n",
        "    num_epochs = 5\n",
        "    learning_rate = 0.005\n",
        "\n",
        "    model = demo_model2()\n",
        "    # model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "    triplet_loss_fn = triplet_loss()\n",
        "\n",
        "    acc_history = {'train': [], 'test': []}\n",
        "    loss_history = {'train': [], 'test': []}\n",
        "\n",
        "    for epoch in trange(num_epochs):\n",
        "        train_acc, train_loss = train_one_epoch2(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy, triplet_loss_fn=triplet_loss_fn)\n",
        "        test_acc, test_loss = test_model2(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy, triplet_loss_fn=triplet_loss_fn)\n",
        "\n",
        "        acc_history['train'].append(train_acc)\n",
        "        acc_history['test'].append(test_acc)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['test'].append(test_loss)\n",
        "\n",
        "    # custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
        "    print(acc_history)\n",
        "    print(loss_history)"
      ],
      "metadata": {
        "id": "7K1am2pgJsZh"
      },
      "id": "7K1am2pgJsZh",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class triplet_loss(nn.Module):\n",
        "    def __init__(self, margin: float = 0.005, device = torch.device('cpu')) -> None:\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        triplet loss implementation, explained in section 3 of project documentation.\n",
        "\n",
        "        Args:\n",
        "            margin: margin parameter in triplet loss (equation 5 in documentation)\n",
        "                default value is 0.005, you may need to set different value for margin depending on your model.\n",
        "            device: pass torch.device(\"cuda:0\") to this parameter if you are using gpu.\n",
        "        \"\"\"\n",
        "        self.margin = torch.tensor(margin)\n",
        "        self.device = device\n",
        "\n",
        "    # Compelete this function\n",
        "    def forward(self, embeddings, labels):\n",
        "        \"\"\"\n",
        "        embeddings: [M, d] M is the batch size, d is the dimension of the embeddings (dimension of the layer before the last layer)\n",
        "        embeddings are supposed to be outputs of the layer before the last layer.\n",
        "\n",
        "        labels: [M, ]\n",
        "        \"\"\"\n",
        "\n",
        "        dp, dn = self.batch_hard_triplet_loss(embeddings, labels)\n",
        "        sigma = 0\n",
        "        for i in range(dp.shape[0]):\n",
        "            result = max(dp[i]-dn[i]+self.margin, 0)\n",
        "            sigma += result\n",
        "\n",
        "        triplet_loss = sigma / labels.shape[0]\n",
        "\n",
        "        return triplet_loss\n",
        "\n",
        "    def batch_hard_triplet_loss(self, embeddings, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embeddings -> [N, d]\n",
        "            labels -> [N, 1]\n",
        "\n",
        "        returns:\n",
        "            dp -> [N, 1] distance of furthest positive pair for each sample\n",
        "            dn -> [N, 1] distance of closest negative pair for each sample\n",
        "        \"\"\"\n",
        "\n",
        "        dists = self.euclidean_dist(embeddings, embeddings)\n",
        "        # dists -> [N, N], square mat of all distances,\n",
        "        # dists[i, j] is distance between sample[i] and sample[j]\n",
        "\n",
        "        same_identity_mask = torch.eq(labels[:, None], labels[None, :])\n",
        "        # [N, N], same_mask[i, j] = True when sample i and j have the same label\n",
        "\n",
        "        negative_mask = torch.logical_not(same_identity_mask)\n",
        "        # [N, N], negative_mask[i, j] = True when sample i and j have different label\n",
        "\n",
        "        positive_mask = torch.logical_xor(same_identity_mask, torch.eye(labels.shape[0], dtype=torch.bool).to(self.device))\n",
        "        # [N, N], same as same_identity mask, except diagonal is zero\n",
        "\n",
        "        dp, _ = torch.max(dists * (positive_mask.int()), dim=1)\n",
        "\n",
        "        dn = torch.zeros_like(dp)\n",
        "        for i in range(dists.shape[0]):\n",
        "            dn[i] = torch.min(dists[i, :][negative_mask[i, :]])\n",
        "\n",
        "        return dp, dn\n",
        "\n",
        "    def all_diffs(self, a, b):\n",
        "        # a, b -> [N, d]\n",
        "        # return -> [N, N, d]\n",
        "        return a[:, None] - b[None, :]\n",
        "\n",
        "    def euclidean_dist(self, embed1, embed2):\n",
        "        # embed1, embed2 -> [N, d]\n",
        "        # return [N, N] -> # get a square matrix of all diffs, diagonal is zero\n",
        "        diffs = self.all_diffs(embed1, embed2)\n",
        "        t1 = torch.square(diffs)\n",
        "        t2 = torch.sum(t1, dim=-1)\n",
        "        return torch.sqrt(t2 + 1e-12)"
      ],
      "metadata": {
        "id": "WWJognxFHMMA"
      },
      "id": "WWJognxFHMMA",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bahTpe3YJtZK",
        "outputId": "7635228a-cb9c-4f6a-9b50-a780de1ee875"
      },
      "id": "bahTpe3YJtZK",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [10:41<00:00, 128.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': [tensor(14.99), tensor(13.39), tensor(13.18), tensor(13.17), tensor(13.17)], 'test': [tensor(14.09), tensor(13.12), tensor(13.11), tensor(13.12), tensor(13.06)]}\n",
            "{'train': [2.3305787821568407, 2.3082051765181615, 2.3001569781476245, 2.2943456294948357, 2.2893106789985445], 'test': [2.5056281498903354, 2.4576464489366883, 2.440762412618603, 2.43770384506361, 2.4433484796941634]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}