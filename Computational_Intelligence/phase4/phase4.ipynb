{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "590f4e95-e064-4e9a-99b5-e576d88953de",
   "metadata": {
    "id": "590f4e95-e064-4e9a-99b5-e576d88953de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.functional import F\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jrxUclDI44gI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrxUclDI44gI",
    "outputId": "5d546ac6-635b-430a-9024-20231f996a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "FnvfBIus5Hww",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnvfBIus5Hww",
    "outputId": "474a26d8-2035-4d8c-95a7-bdb0fe5e7bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1olR9djsPqRY8dxn01k2qeMnqfOfqWRQE/phase4 public\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/drive/MyDrive/Colab Notebooks/phase4 public'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dZaJuwrUu6Xx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZaJuwrUu6Xx",
    "outputId": "97c80642-cf75-461a-fa52-7283b84a093f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CI - Spring_2023 - phase4.pdf'   exploring_data.py   utils.py\n",
      " data\t\t\t\t  __pycache__\n",
      " dataloader_demo.ipynb\t\t  triplet_loss.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Acv3G2Er5LAG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Acv3G2Er5LAG",
    "outputId": "9d21d88b-21a4-48cf-804d-c03e2c749aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datafiles to read:  {'train': './data/12000_train_mnistmnistmsvhnsynusps.npz', 'test': './data/12000_test_mnistmnistmsvhnsynusps.npz'}\n",
      "reading ./data/12000_train_mnistmnistmsvhnsynusps.npz, number of samples: 60000\n",
      "reading ./data/12000_test_mnistmnistmsvhnsynusps.npz, number of samples: 21600\n",
      "reading ./data/12000_test_mnistmnistmsvhnsynusps.npz, number of samples: 21600\n",
      "dict_keys(['train', 'test', 'test_missing', 'train_size', 'test_size', 'test_missing_size'])\n",
      "0-th batch\n",
      "images shape:  torch.Size([128, 3, 32, 32])\n",
      "features shape:  torch.Size([128, 256])\n",
      "domain labels freq:  (tensor([0, 1, 2, 3, 4]), tensor([22, 25, 20, 35, 26]))\n",
      "digit labels freq:  (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([11, 14, 14, 14, 13, 17, 13, 11, 13,  8]))\n",
      "\n",
      "0-th batch\n",
      "images shape:  torch.Size([128, 3, 32, 32])\n",
      "features shape:  torch.Size([128, 256])\n",
      "domain labels freq:  (tensor([0, 1, 2, 3, 4]), tensor([22, 18, 26, 31, 31]))\n",
      "digit labels freq:  (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([10, 20, 13, 10,  9, 15, 11, 10, 15, 15]))\n",
      "\n",
      "0-th batch\n",
      "images shape:  torch.Size([128, 3, 32, 32])\n",
      "features shape:  torch.Size([128, 256])\n",
      "in test-missing dataloaders, since the features are not available, features are filled with zeros tensor(0.)\n",
      "domain labels freq:  (tensor([0, 1, 2, 3, 4]), tensor([35, 18, 26, 23, 26]))\n",
      "digit labels freq:  (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([13, 19, 17,  9, 14, 17, 12,  9, 10,  8]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import get_data_loaders\n",
    "import torch\n",
    "\n",
    "def loaders_demo():\n",
    "    full_dataloaders, _ = get_data_loaders(\n",
    "        filenames={\n",
    "            'train': './data/12000_train_mnistmnistmsvhnsynusps.npz',\n",
    "            'test': './data/12000_test_mnistmnistmsvhnsynusps.npz',\n",
    "        },\n",
    "        batch_size= 128\n",
    "    )\n",
    "    return full_dataloaders\n",
    "\n",
    "full_dataloaders = loaders_demo()\n",
    "print(full_dataloaders.keys())\n",
    "\n",
    "for phase in ['train', 'test', 'test_missing']:\n",
    "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(full_dataloaders[phase]):\n",
    "        print(f'{batch_indx}-th batch')\n",
    "        print('images shape: ', images.shape)\n",
    "        print('features shape: ', features.shape)\n",
    "        if phase == 'test_missing':\n",
    "            print('in test-missing dataloaders, since the features are not available, features are filled with zeros', torch.sum(features))\n",
    "        print('domain labels freq: ', torch.unique(domain_labels, return_counts=True))\n",
    "        print('digit labels freq: ', torch.unique(digit_labels, return_counts=True))\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4e4b9a-b271-4258-9205-657ffbf511fb",
   "metadata": {
    "id": "8c4e4b9a-b271-4258-9205-657ffbf511fb"
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sv5HautEdocj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sv5HautEdocj",
    "outputId": "8ca8e2b5-bc52-4374-d712-70b6bc12c547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_gpu = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08975e50-8276-48d0-bfe5-38164ec80855",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08975e50-8276-48d0-bfe5-38164ec80855",
    "outputId": "4b12783b-f6c7-48f2-ba23-9b004b8bc94e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 21600)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dataloaders['train'].dataset), len(full_dataloaders['test'].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e11e3-1407-4deb-bc24-e74bf63bb639",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "058e11e3-1407-4deb-bc24-e74bf63bb639",
    "outputId": "c5909523-ade7-4568-e738-23969a840ce6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e97df8c2-7f30-4dda-a26d-9e18023f5eb1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.037965</td>\n",
       "      <td>-1.799968</td>\n",
       "      <td>1.182611</td>\n",
       "      <td>-0.248518</td>\n",
       "      <td>-0.320674</td>\n",
       "      <td>0.251610</td>\n",
       "      <td>0.734077</td>\n",
       "      <td>0.608663</td>\n",
       "      <td>0.212084</td>\n",
       "      <td>0.767771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072638</td>\n",
       "      <td>0.316827</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>-0.329847</td>\n",
       "      <td>-0.159941</td>\n",
       "      <td>-0.212282</td>\n",
       "      <td>0.070331</td>\n",
       "      <td>-0.110758</td>\n",
       "      <td>-0.065389</td>\n",
       "      <td>-0.018419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.564800</td>\n",
       "      <td>0.517854</td>\n",
       "      <td>1.038122</td>\n",
       "      <td>-0.489715</td>\n",
       "      <td>-1.271697</td>\n",
       "      <td>-0.718387</td>\n",
       "      <td>2.967587</td>\n",
       "      <td>3.289755</td>\n",
       "      <td>0.988581</td>\n",
       "      <td>-0.101411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.454926</td>\n",
       "      <td>-0.120079</td>\n",
       "      <td>-0.101010</td>\n",
       "      <td>-0.171803</td>\n",
       "      <td>-0.297529</td>\n",
       "      <td>0.160597</td>\n",
       "      <td>-0.018645</td>\n",
       "      <td>-0.045893</td>\n",
       "      <td>-0.045439</td>\n",
       "      <td>-0.096207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.445361</td>\n",
       "      <td>-0.557834</td>\n",
       "      <td>-1.135024</td>\n",
       "      <td>0.368366</td>\n",
       "      <td>0.614173</td>\n",
       "      <td>0.158659</td>\n",
       "      <td>-0.522470</td>\n",
       "      <td>-0.589508</td>\n",
       "      <td>0.290434</td>\n",
       "      <td>-0.124693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135883</td>\n",
       "      <td>-0.080708</td>\n",
       "      <td>0.098077</td>\n",
       "      <td>-0.099581</td>\n",
       "      <td>0.086981</td>\n",
       "      <td>-0.015576</td>\n",
       "      <td>0.033817</td>\n",
       "      <td>-0.171754</td>\n",
       "      <td>-0.028527</td>\n",
       "      <td>-0.012392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.744837</td>\n",
       "      <td>-4.175048</td>\n",
       "      <td>0.368809</td>\n",
       "      <td>0.091318</td>\n",
       "      <td>1.096337</td>\n",
       "      <td>0.079871</td>\n",
       "      <td>0.053634</td>\n",
       "      <td>-0.415834</td>\n",
       "      <td>-0.559677</td>\n",
       "      <td>0.074155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032611</td>\n",
       "      <td>-0.089508</td>\n",
       "      <td>-0.032700</td>\n",
       "      <td>-0.127731</td>\n",
       "      <td>-0.191442</td>\n",
       "      <td>-0.135071</td>\n",
       "      <td>-0.055170</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>-0.098564</td>\n",
       "      <td>0.124951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.228644</td>\n",
       "      <td>-3.329779</td>\n",
       "      <td>-2.187275</td>\n",
       "      <td>3.250448</td>\n",
       "      <td>4.089767</td>\n",
       "      <td>-0.171248</td>\n",
       "      <td>-0.909210</td>\n",
       "      <td>-0.898119</td>\n",
       "      <td>-4.993124</td>\n",
       "      <td>-0.198844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128845</td>\n",
       "      <td>0.038883</td>\n",
       "      <td>-0.115404</td>\n",
       "      <td>0.339894</td>\n",
       "      <td>-0.298737</td>\n",
       "      <td>-0.154749</td>\n",
       "      <td>-0.035553</td>\n",
       "      <td>0.143919</td>\n",
       "      <td>0.050182</td>\n",
       "      <td>0.179309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-14.531523</td>\n",
       "      <td>-6.808526</td>\n",
       "      <td>-1.962304</td>\n",
       "      <td>-2.109931</td>\n",
       "      <td>2.470517</td>\n",
       "      <td>0.892024</td>\n",
       "      <td>1.383679</td>\n",
       "      <td>-2.025483</td>\n",
       "      <td>-0.928582</td>\n",
       "      <td>0.343404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074560</td>\n",
       "      <td>0.105696</td>\n",
       "      <td>-0.235819</td>\n",
       "      <td>-0.009158</td>\n",
       "      <td>0.068220</td>\n",
       "      <td>0.018031</td>\n",
       "      <td>0.079444</td>\n",
       "      <td>-0.026290</td>\n",
       "      <td>0.098710</td>\n",
       "      <td>-0.030104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>8.207400</td>\n",
       "      <td>5.156352</td>\n",
       "      <td>-3.013297</td>\n",
       "      <td>0.255962</td>\n",
       "      <td>2.002417</td>\n",
       "      <td>-2.155067</td>\n",
       "      <td>4.849430</td>\n",
       "      <td>0.571528</td>\n",
       "      <td>3.321007</td>\n",
       "      <td>-1.156567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072982</td>\n",
       "      <td>-0.017911</td>\n",
       "      <td>0.056130</td>\n",
       "      <td>-0.089698</td>\n",
       "      <td>-0.018625</td>\n",
       "      <td>0.021709</td>\n",
       "      <td>-0.126845</td>\n",
       "      <td>0.075862</td>\n",
       "      <td>0.179969</td>\n",
       "      <td>0.049354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-11.028695</td>\n",
       "      <td>2.798279</td>\n",
       "      <td>-1.152583</td>\n",
       "      <td>-0.779837</td>\n",
       "      <td>3.369474</td>\n",
       "      <td>-0.146005</td>\n",
       "      <td>-0.842274</td>\n",
       "      <td>-4.763255</td>\n",
       "      <td>6.170467</td>\n",
       "      <td>-2.651726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179539</td>\n",
       "      <td>0.107593</td>\n",
       "      <td>-0.226426</td>\n",
       "      <td>0.093342</td>\n",
       "      <td>-0.087881</td>\n",
       "      <td>0.130228</td>\n",
       "      <td>0.142368</td>\n",
       "      <td>0.333962</td>\n",
       "      <td>0.225531</td>\n",
       "      <td>0.132795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-10.087826</td>\n",
       "      <td>4.747341</td>\n",
       "      <td>0.836692</td>\n",
       "      <td>-5.564118</td>\n",
       "      <td>-1.667153</td>\n",
       "      <td>-1.559360</td>\n",
       "      <td>-3.061188</td>\n",
       "      <td>-0.031718</td>\n",
       "      <td>2.186175</td>\n",
       "      <td>1.350780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024626</td>\n",
       "      <td>-0.005803</td>\n",
       "      <td>-0.003717</td>\n",
       "      <td>-0.054994</td>\n",
       "      <td>-0.009849</td>\n",
       "      <td>0.031435</td>\n",
       "      <td>-0.002613</td>\n",
       "      <td>0.011187</td>\n",
       "      <td>-0.032867</td>\n",
       "      <td>0.009332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-12.342452</td>\n",
       "      <td>-0.540276</td>\n",
       "      <td>-0.401978</td>\n",
       "      <td>0.312520</td>\n",
       "      <td>-4.852377</td>\n",
       "      <td>-1.126977</td>\n",
       "      <td>-0.722765</td>\n",
       "      <td>0.111993</td>\n",
       "      <td>-1.917904</td>\n",
       "      <td>-0.856894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012340</td>\n",
       "      <td>-0.019494</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>-0.004061</td>\n",
       "      <td>0.043480</td>\n",
       "      <td>0.011145</td>\n",
       "      <td>-0.016540</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>-0.016897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 256 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e97df8c2-7f30-4dda-a26d-9e18023f5eb1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e97df8c2-7f30-4dda-a26d-9e18023f5eb1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e97df8c2-7f30-4dda-a26d-9e18023f5eb1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     6.037965 -1.799968  1.182611 -0.248518 -0.320674  0.251610  0.734077   \n",
       "1   -12.564800  0.517854  1.038122 -0.489715 -1.271697 -0.718387  2.967587   \n",
       "2     7.445361 -0.557834 -1.135024  0.368366  0.614173  0.158659 -0.522470   \n",
       "3     0.744837 -4.175048  0.368809  0.091318  1.096337  0.079871  0.053634   \n",
       "4     0.228644 -3.329779 -2.187275  3.250448  4.089767 -0.171248 -0.909210   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "123 -14.531523 -6.808526 -1.962304 -2.109931  2.470517  0.892024  1.383679   \n",
       "124   8.207400  5.156352 -3.013297  0.255962  2.002417 -2.155067  4.849430   \n",
       "125 -11.028695  2.798279 -1.152583 -0.779837  3.369474 -0.146005 -0.842274   \n",
       "126 -10.087826  4.747341  0.836692 -5.564118 -1.667153 -1.559360 -3.061188   \n",
       "127 -12.342452 -0.540276 -0.401978  0.312520 -4.852377 -1.126977 -0.722765   \n",
       "\n",
       "          7         8         9    ...       246       247       248  \\\n",
       "0    0.608663  0.212084  0.767771  ... -0.072638  0.316827  0.023135   \n",
       "1    3.289755  0.988581 -0.101411  ... -0.454926 -0.120079 -0.101010   \n",
       "2   -0.589508  0.290434 -0.124693  ... -0.135883 -0.080708  0.098077   \n",
       "3   -0.415834 -0.559677  0.074155  ... -0.032611 -0.089508 -0.032700   \n",
       "4   -0.898119 -4.993124 -0.198844  ...  0.128845  0.038883 -0.115404   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123 -2.025483 -0.928582  0.343404  ...  0.074560  0.105696 -0.235819   \n",
       "124  0.571528  3.321007 -1.156567  ...  0.072982 -0.017911  0.056130   \n",
       "125 -4.763255  6.170467 -2.651726  ... -0.179539  0.107593 -0.226426   \n",
       "126 -0.031718  2.186175  1.350780  ...  0.024626 -0.005803 -0.003717   \n",
       "127  0.111993 -1.917904 -0.856894  ...  0.012340 -0.019494  0.009521   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0   -0.329847 -0.159941 -0.212282  0.070331 -0.110758 -0.065389 -0.018419  \n",
       "1   -0.171803 -0.297529  0.160597 -0.018645 -0.045893 -0.045439 -0.096207  \n",
       "2   -0.099581  0.086981 -0.015576  0.033817 -0.171754 -0.028527 -0.012392  \n",
       "3   -0.127731 -0.191442 -0.135071 -0.055170  0.002007 -0.098564  0.124951  \n",
       "4    0.339894 -0.298737 -0.154749 -0.035553  0.143919  0.050182  0.179309  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123 -0.009158  0.068220  0.018031  0.079444 -0.026290  0.098710 -0.030104  \n",
       "124 -0.089698 -0.018625  0.021709 -0.126845  0.075862  0.179969  0.049354  \n",
       "125  0.093342 -0.087881  0.130228  0.142368  0.333962  0.225531  0.132795  \n",
       "126 -0.054994 -0.009849  0.031435 -0.002613  0.011187 -0.032867  0.009332  \n",
       "127 -0.004061  0.043480  0.011145 -0.016540  0.011772  0.004417 -0.016897  \n",
       "\n",
       "[128 rows x 256 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(full_dataloaders['train']):\n",
    "    if batch_indx < 1:\n",
    "        break\n",
    "import pandas as pd\n",
    "pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a158c4-ece8-4c48-a279-b989b799fbef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4a158c4-ece8-4c48-a279-b989b799fbef",
    "outputId": "cbc6beb0-823b-4099-9a2c-2d07916bd89d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 469)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_indx, len(full_dataloaders['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10895013-9a4a-4d8b-95c8-905022821dbc",
   "metadata": {
    "id": "10895013-9a4a-4d8b-95c8-905022821dbc"
   },
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b329db-4b19-4472-946e-6ee4e19f9b2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2b329db-4b19-4472-946e-6ee4e19f9b2f",
    "outputId": "f0bfaa14-d468-4e0c-b0a1-7f80432e9170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_model(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=3456, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class demo_model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, 5, stride=1, padding=0)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3, stride=1, padding=0)\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=0)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=3200 + 256, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs, inputs2, debug=False):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(inputs)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 1)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 1)\n",
    "        x0 = self.flatten(x)\n",
    "\n",
    "        use_gpu = True\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        merged_input = torch.zeros(128, 3456)\n",
    "        merged_input = merged_input.to(device)\n",
    "\n",
    "        for i in range(x0.shape[0]):\n",
    "            merged_input[i] = torch.cat((x0[i], inputs2[i]))\n",
    "\n",
    "\n",
    "        x1 = self.fc1(merged_input)\n",
    "        x1 = F.relu(x1)\n",
    "        x2 = self.fc2(x1)\n",
    "        x2 = F.relu(x2)\n",
    "        outputs = self.fc3(x2)\n",
    "\n",
    "        if debug:\n",
    "            print('inputs shape: ', inputs.shape) # inputs in shape [N, C, H, W]\n",
    "            print('after flattening: ', x0.shape)\n",
    "            print('Activations after 1st fully connected layer: ', x1.shape)\n",
    "            print('Activations after 2nd fully connected layer: ', x2.shape)\n",
    "            print('Output shape: ', outputs.shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "model = demo_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26ebd8-98a0-4875-aec5-e54d103cfc13",
   "metadata": {
    "id": "0f26ebd8-98a0-4875-aec5-e54d103cfc13"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model: nn.Module, optim: torch.optim.Optimizer,\n",
    "         dataloader: DataLoader, loss_fn):\n",
    "\n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train() #\n",
    "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
    "        if digit_labels.shape[0] != 128:\n",
    "            continue\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        digit_labels = digit_labels.to(device)\n",
    "\n",
    "        outputs = model(images, features) # Forward Pass, [N, 10]\n",
    "        loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
    "\n",
    "        loss.backward() # Compute Gradients\n",
    "        optim.step() # Update parameters\n",
    "        optim.zero_grad() # zero the parameter's gradients\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1) # Explain, [N]\n",
    "        running_corrects += torch.sum(preds == digit_labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_acc = (running_corrects / num_samples) * 100\n",
    "    epoch_loss = (running_loss / num_batches)\n",
    "\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042f4bc-af39-4fe9-9ac5-04a03e9a7d5d",
   "metadata": {
    "id": "1042f4bc-af39-4fe9-9ac5-04a03e9a7d5d"
   },
   "source": [
    "## Evaluating Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90956779-3d38-41ab-924e-e2c6ac6c6671",
   "metadata": {
    "id": "90956779-3d38-41ab-924e-e2c6ac6c6671"
   },
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module,\n",
    "         dataloader: DataLoader, loss_fn):\n",
    "\n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "    with torch.no_grad():\n",
    "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
    "            if digit_labels.shape[0] != 128:\n",
    "                continue\n",
    "            images = images.to(device)\n",
    "            features = features.to(device)\n",
    "            digit_labels = digit_labels.to(device)\n",
    "\n",
    "            outputs = model(images, features) # Forward Pass\n",
    "            loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
    "\n",
    "            _, preds = torch.max(outputs, 1) #\n",
    "            running_corrects += torch.sum(preds == digit_labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    test_acc = (running_corrects / num_samples) * 100\n",
    "    test_loss = (running_loss / num_batches)\n",
    "\n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fMbOQp5B5Ql1",
   "metadata": {
    "id": "fMbOQp5B5Ql1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def custom_plot_training_stats(acc_hist, loss_hist, phase_list, title: str, dir: str):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=[14, 6], dpi=100)\n",
    "\n",
    "    for phase in phase_list:\n",
    "        lowest_loss_x = np.argmin(np.array(loss_hist[phase]))\n",
    "        lowest_loss_y = loss_hist[phase][lowest_loss_x]\n",
    "\n",
    "        ax1.annotate(\"{:.4f}\".format(lowest_loss_y), [lowest_loss_x, lowest_loss_y])\n",
    "        ax1.plot(loss_hist[phase], '-x', label=f'{phase} loss', markevery = [lowest_loss_x])\n",
    "\n",
    "        ax1.set_xlabel(xlabel='epochs')\n",
    "        ax1.set_ylabel(ylabel='loss')\n",
    "\n",
    "        ax1.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
    "        ax1.legend()\n",
    "        ax1.label_outer()\n",
    "\n",
    "    # acc:\n",
    "    for phase in phase_list:\n",
    "        highest_acc_x = np.argmax(np.array(acc_hist[phase].cpu()))\n",
    "        highest_acc_y = acc_hist[phase][highest_acc_x]\n",
    "\n",
    "        ax2.annotate(\"{:.4f}\".format(highest_acc_y), [highest_acc_x, highest_acc_y])\n",
    "        ax2.plot(acc_hist[phase], '-x', label=f'{phase} loss', markevery = [highest_acc_x])\n",
    "\n",
    "        ax2.set_xlabel(xlabel='epochs')\n",
    "        ax2.set_ylabel(ylabel='acc')\n",
    "\n",
    "        ax2.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
    "        ax2.legend()\n",
    "        #ax2.label_outer()\n",
    "\n",
    "    fig.suptitle(f'{title}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd46bbd-532a-4f57-a027-34a762c191de",
   "metadata": {
    "id": "4fd46bbd-532a-4f57-a027-34a762c191de"
   },
   "outputs": [],
   "source": [
    "def demo():\n",
    "    batch_size = 128\n",
    "    num_epochs = 25\n",
    "    learning_rate = 0.005\n",
    "\n",
    "    model = demo_model()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    acc_history = {'train': [], 'test': []}\n",
    "    loss_history = {'train': [], 'test': []}\n",
    "\n",
    "    for epoch in trange(num_epochs):\n",
    "        train_acc, train_loss = train_one_epoch(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
    "        test_acc, test_loss = test_model(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
    "\n",
    "        acc_history['train'].append(train_acc)\n",
    "        acc_history['test'].append(test_acc)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        loss_history['test'].append(test_loss)\n",
    "\n",
    "    # custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
    "    print(acc_history)\n",
    "    print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a03b45-5d6f-4699-a308-f335c17d0a8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8a03b45-5d6f-4699-a308-f335c17d0a8f",
    "outputId": "ec6b25b1-d4dd-4eda-f4b9-1278ef70905b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [05:47<00:00, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [tensor(19.46, device='cuda:0'), tensor(27.77, device='cuda:0'), tensor(36.31, device='cuda:0'), tensor(44.98, device='cuda:0'), tensor(50.22, device='cuda:0'), tensor(54.23, device='cuda:0'), tensor(59.08, device='cuda:0'), tensor(64.84, device='cuda:0'), tensor(70.45, device='cuda:0'), tensor(74.50, device='cuda:0'), tensor(76.86, device='cuda:0'), tensor(78.60, device='cuda:0'), tensor(79.91, device='cuda:0'), tensor(81.10, device='cuda:0'), tensor(82.07, device='cuda:0'), tensor(83.12, device='cuda:0'), tensor(83.90, device='cuda:0'), tensor(84.87, device='cuda:0'), tensor(85.55, device='cuda:0'), tensor(86.34, device='cuda:0'), tensor(87.03, device='cuda:0'), tensor(87.62, device='cuda:0'), tensor(88.11, device='cuda:0'), tensor(88.67, device='cuda:0'), tensor(89.11, device='cuda:0')], 'test': [tensor(24.67, device='cuda:0'), tensor(30.42, device='cuda:0'), tensor(40.53, device='cuda:0'), tensor(47.17, device='cuda:0'), tensor(51.62, device='cuda:0'), tensor(56.35, device='cuda:0'), tensor(61.81, device='cuda:0'), tensor(67.32, device='cuda:0'), tensor(72.93, device='cuda:0'), tensor(75.94, device='cuda:0'), tensor(78.40, device='cuda:0'), tensor(79.22, device='cuda:0'), tensor(79.31, device='cuda:0'), tensor(81.48, device='cuda:0'), tensor(82.28, device='cuda:0'), tensor(83.16, device='cuda:0'), tensor(83.46, device='cuda:0'), tensor(84.47, device='cuda:0'), tensor(85.30, device='cuda:0'), tensor(85.64, device='cuda:0'), tensor(85.99, device='cuda:0'), tensor(86.44, device='cuda:0'), tensor(87.25, device='cuda:0'), tensor(87.38, device='cuda:0'), tensor(87.83, device='cuda:0')]}\n",
      "{'train': [2.2637418953340447, 2.1453863984740367, 1.9419992938478872, 1.7026492235248785, 1.5145221409767167, 1.3704775508278724, 1.2425774113455814, 1.093186024790888, 0.9367119137412195, 0.8153700632835502, 0.7383021126423818, 0.6848870801773153, 0.64486141181958, 0.6067517169757184, 0.5762182271429724, 0.5458443994079826, 0.5180937189664414, 0.49094466398011394, 0.46795958020031325, 0.4441261621617051, 0.4226697557516444, 0.4028555257106895, 0.38448418279700697, 0.3684670333859763, 0.35208226905575696], 'test': [2.211622764372967, 2.046048403491635, 1.8185519678352853, 1.5964362057003043, 1.4393199922065059, 1.2996822835425654, 1.1659424227370312, 1.0003051299315233, 0.8540453826181987, 0.7674455353494226, 0.6963427941474689, 0.6632381347862221, 0.661501119651738, 0.5918482681350595, 0.5710721268103673, 0.5416108720754025, 0.5257693989389747, 0.49203871481517364, 0.46912011380731705, 0.4596804050651528, 0.444429853787789, 0.42767372833201167, 0.41519693827488013, 0.4037855711032653, 0.39574852956117257]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Aff4PEsJBnh",
   "metadata": {
    "id": "_Aff4PEsJBnh"
   },
   "source": [
    "# part 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sF5_pzlLG9qB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sF5_pzlLG9qB",
    "outputId": "59759bc5-afbf-4b26-8f4f-60495a56d202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_model2(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=3456, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class demo_model2(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, 5, stride=1, padding=0)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3, stride=1, padding=0)\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=0)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=3200 + 256, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs, inputs2, debug=False):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(inputs)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 1)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 1)\n",
    "        x0 = self.flatten(x)\n",
    "\n",
    "        use_gpu = True\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        merged_input = torch.zeros(128, 3456)\n",
    "        merged_input = merged_input.to(device)\n",
    "\n",
    "        for i in range(x0.shape[0]):\n",
    "            merged_input[i] = torch.cat((x0[i], inputs2[i]))\n",
    "\n",
    "\n",
    "        x1 = self.fc1(merged_input)\n",
    "        x1 = F.relu(x1)\n",
    "        x2 = self.fc2(x1)\n",
    "        penultimate = F.relu(x2)\n",
    "        outputs = self.fc3(penultimate)\n",
    "\n",
    "        if debug:\n",
    "            print('inputs shape: ', inputs.shape) # inputs in shape [N, C, H, W]\n",
    "            print('after flattening: ', x0.shape)\n",
    "            print('Activations after 1st fully connected layer: ', x1.shape)\n",
    "            print('Activations after 2nd fully connected layer: ', x2.shape)\n",
    "            print('Output shape: ', outputs.shape)\n",
    "\n",
    "        return outputs, penultimate\n",
    "\n",
    "model = demo_model2()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "SriUjNHHEysw",
   "metadata": {
    "id": "SriUjNHHEysw"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch2(model: nn.Module, optim: torch.optim.Optimizer,\n",
    "         dataloader: DataLoader, loss_fn, triplet_loss_fn):\n",
    "\n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train() #\n",
    "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
    "        if digit_labels.shape[0] != 128:\n",
    "            continue\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        digit_labels = digit_labels.to(device)\n",
    "        domain_labels = domain_labels.to(device)\n",
    "\n",
    "        outputs, penultimate = model(images, features) # Forward Pass, [N, 10]\n",
    "        loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
    "\n",
    "        loss2 = triplet_loss_fn(penultimate, domain_labels)\n",
    "        landa = 0.1\n",
    "        loss_total = loss + landa * loss2\n",
    "\n",
    "        loss_total.backward() # Compute Gradients\n",
    "        optim.step() # Update parameters\n",
    "        optim.zero_grad() # zero the parameter's gradients\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1) # Explain, [N]\n",
    "        running_corrects += torch.sum(preds == digit_labels)\n",
    "        running_loss += loss_total.item()\n",
    "\n",
    "    epoch_acc = (running_corrects / num_samples) * 100\n",
    "    epoch_loss = (running_loss / num_batches)\n",
    "\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pgSqvfXRJftZ",
   "metadata": {
    "id": "pgSqvfXRJftZ"
   },
   "source": [
    "## Evaluating Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "LFDhQmnCJfta",
   "metadata": {
    "id": "LFDhQmnCJfta"
   },
   "outputs": [],
   "source": [
    "def test_model2(model: nn.Module,\n",
    "         dataloader: DataLoader, loss_fn, triplet_loss_fn):\n",
    "\n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "    with torch.no_grad():\n",
    "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
    "            if digit_labels.shape[0] != 128:\n",
    "                continue\n",
    "            images = images.to(device)\n",
    "            features = features.to(device)\n",
    "            digit_labels = digit_labels.to(device)\n",
    "            domain_labels = domain_labels.to(device)\n",
    "\n",
    "            outputs, penultimate = model(images, features) # Forward Pass\n",
    "            loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
    "            loss2 = triplet_loss_fn(penultimate, domain_labels)\n",
    "            print(loss2)\n",
    "            landa = 0.1\n",
    "            loss_total = loss + landa * loss2\n",
    "\n",
    "            _, preds = torch.max(outputs, 1) #\n",
    "            running_corrects += torch.sum(preds == digit_labels)\n",
    "            running_loss += loss_total.item()\n",
    "\n",
    "    test_acc = (running_corrects / num_samples) * 100\n",
    "    test_loss = (running_loss / num_batches)\n",
    "\n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "WWJognxFHMMA",
   "metadata": {
    "id": "WWJognxFHMMA"
   },
   "outputs": [],
   "source": [
    "class triplet_loss(nn.Module):\n",
    "    def __init__(self, margin: float = 0.005, device = torch.device(\"cuda\")) -> None:\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        triplet loss implementation, explained in section 3 of project documentation.\n",
    "\n",
    "        Args:\n",
    "            margin: margin parameter in triplet loss (equation 5 in documentation)\n",
    "                default value is 0.005, you may need to set different value for margin depending on your model.\n",
    "            device: pass torch.device(\"cuda:0\") to this parameter if you are using gpu.\n",
    "        \"\"\"\n",
    "        self.margin = torch.tensor(margin)\n",
    "        self.device = device\n",
    "\n",
    "    # Compelete this function\n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        embeddings: [M, d] M is the batch size, d is the dimension of the embeddings (dimension of the layer before the last layer)\n",
    "        embeddings are supposed to be outputs of the layer before the last layer.\n",
    "\n",
    "        labels: [M, ]\n",
    "        \"\"\"\n",
    "\n",
    "        dp, dn = self.batch_hard_triplet_loss(embeddings, labels)\n",
    "        sigma = 0\n",
    "        for i in range(dp.shape[0]):\n",
    "            result = max(dp[i]-dn[i]+self.margin, 0)\n",
    "            sigma += result\n",
    "\n",
    "        triplet_loss = sigma / labels.shape[0]\n",
    "\n",
    "        return triplet_loss\n",
    "\n",
    "    def batch_hard_triplet_loss(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings -> [N, d]\n",
    "            labels -> [N, 1]\n",
    "\n",
    "        returns:\n",
    "            dp -> [N, 1] distance of furthest positive pair for each sample\n",
    "            dn -> [N, 1] distance of closest negative pair for each sample\n",
    "        \"\"\"\n",
    "\n",
    "        dists = self.euclidean_dist(embeddings, embeddings)\n",
    "        # dists -> [N, N], square mat of all distances,\n",
    "        # dists[i, j] is distance between sample[i] and sample[j]\n",
    "\n",
    "        same_identity_mask = torch.eq(labels[:, None], labels[None, :])\n",
    "        # [N, N], same_mask[i, j] = True when sample i and j have the same label\n",
    "\n",
    "        negative_mask = torch.logical_not(same_identity_mask)\n",
    "        # [N, N], negative_mask[i, j] = True when sample i and j have different label\n",
    "\n",
    "        positive_mask = torch.logical_xor(same_identity_mask, torch.eye(labels.shape[0], dtype=torch.bool).to(self.device))\n",
    "        # [N, N], same as same_identity mask, except diagonal is zero\n",
    "\n",
    "        dp, _ = torch.max(dists * (positive_mask.int()), dim=1)\n",
    "\n",
    "        dn = torch.zeros_like(dp)\n",
    "        for i in range(dists.shape[0]):\n",
    "            dn[i] = torch.min(dists[i, :][negative_mask[i, :]])\n",
    "\n",
    "        return dp, dn\n",
    "\n",
    "    def all_diffs(self, a, b):\n",
    "        # a, b -> [N, d]\n",
    "        # return -> [N, N, d]\n",
    "        return a[:, None] - b[None, :]\n",
    "\n",
    "    def euclidean_dist(self, embed1, embed2):\n",
    "        # embed1, embed2 -> [N, d]\n",
    "        # return [N, N] -> # get a square matrix of all diffs, diagonal is zero\n",
    "        diffs = self.all_diffs(embed1, embed2)\n",
    "        t1 = torch.square(diffs)\n",
    "        t2 = torch.sum(t1, dim=-1)\n",
    "        return torch.sqrt(t2 + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7K1am2pgJsZh",
   "metadata": {
    "id": "7K1am2pgJsZh"
   },
   "outputs": [],
   "source": [
    "def demo2():\n",
    "    batch_size = 128\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.005\n",
    "\n",
    "    model = demo_model2()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    triplet_loss_fn = triplet_loss()\n",
    "\n",
    "    acc_history = {'train': [], 'test': []}\n",
    "    loss_history = {'train': [], 'test': []}\n",
    "\n",
    "    for epoch in trange(num_epochs):\n",
    "        train_acc, train_loss = train_one_epoch2(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy, triplet_loss_fn=triplet_loss_fn)\n",
    "        test_acc, test_loss = test_model2(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy, triplet_loss_fn=triplet_loss_fn)\n",
    "\n",
    "        acc_history['train'].append(train_acc)\n",
    "        acc_history['test'].append(test_acc)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        loss_history['test'].append(test_loss)\n",
    "\n",
    "    # custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
    "    print(acc_history)\n",
    "    print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bahTpe3YJtZK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bahTpe3YJtZK",
    "outputId": "84469e32-3f50-4cc1-87c3-9f32d70fce01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:40<00:00, 59.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [tensor(18.70, device='cuda:0'), tensor(26.69, device='cuda:0'), tensor(32.22, device='cuda:0'), tensor(36.97, device='cuda:0'), tensor(41.67, device='cuda:0'), tensor(46.64, device='cuda:0'), tensor(51.13, device='cuda:0'), tensor(54.53, device='cuda:0'), tensor(57.65, device='cuda:0'), tensor(60.86, device='cuda:0'), tensor(64.57, device='cuda:0'), tensor(68.79, device='cuda:0'), tensor(73.55, device='cuda:0'), tensor(77.37, device='cuda:0'), tensor(79.95, device='cuda:0'), tensor(82.45, device='cuda:0'), tensor(83.99, device='cuda:0'), tensor(85.28, device='cuda:0'), tensor(86.66, device='cuda:0'), tensor(87.58, device='cuda:0')], 'test': [tensor(23.70, device='cuda:0'), tensor(29.17, device='cuda:0'), tensor(34.06, device='cuda:0'), tensor(37.90, device='cuda:0'), tensor(42.79, device='cuda:0'), tensor(49.08, device='cuda:0'), tensor(52.92, device='cuda:0'), tensor(55.17, device='cuda:0'), tensor(60.36, device='cuda:0'), tensor(62.84, device='cuda:0'), tensor(66.94, device='cuda:0'), tensor(72.00, device='cuda:0'), tensor(75.82, device='cuda:0'), tensor(79.06, device='cuda:0'), tensor(80.20, device='cuda:0'), tensor(82.52, device='cuda:0'), tensor(84.17, device='cuda:0'), tensor(83.78, device='cuda:0'), tensor(85.30, device='cuda:0'), tensor(86.33, device='cuda:0')]}\n",
      "{'train': [2.2997752496682757, 2.2719543808813034, 2.23999355088419, 2.189229160483712, 2.1059971229353946, 1.9937568181105005, 1.880240985071227, 1.7831447053311476, 1.6954793078558785, 1.6071239661560384, 1.507762199525894, 1.3888051598819333, 1.2568441863253172, 1.1325165241766078, 1.030234392136653, 0.9376824224935666, 0.8638884794991663, 0.7988064629690987, 0.741313761485411, 0.6921500318340147], 'test': [2.2760409617565087, 2.2503153149192854, 2.212450460569393, 2.1493898693626448, 2.05155602432567, 1.9364356881768041, 1.831988850994223, 1.7415732102986623, 1.6550297553722675, 1.5594968231472037, 1.4517451256689942, 1.3240658905379166, 1.1859935218765891, 1.082462271995093, 1.0114920791789626, 0.9187226655215202, 0.8500850877818271, 0.82740591298899, 0.7622032602863199, 0.7145463923025414]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "demo2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b-40LJ-t2viA",
   "metadata": {
    "id": "b-40LJ-t2viA"
   },
   "source": [
    "# Bonus score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "KcmdqgD32pbZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcmdqgD32pbZ",
    "outputId": "1c74490a-210c-4407-85ab-45b7ad00618c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_producer_model(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=3200, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class feature_producer_model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, 5, stride=1, padding=0)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3, stride=1, padding=0)\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=0)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=3200, out_features=256)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs, debug=False):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(inputs)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 1)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 1)\n",
    "        x0 = self.flatten(x)\n",
    "        outputs = self.fc1(x0)\n",
    "\n",
    "        if debug:\n",
    "            print('inputs shape: ', inputs.shape) # inputs in shape [N, C, H, W]\n",
    "            print('after flattening: ', x0.shape)\n",
    "            print('Activations after 1st fully connected layer: ', x1.shape)\n",
    "            print('Activations after 2nd fully connected layer: ', x2.shape)\n",
    "            print('Output shape: ', outputs.shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "model = feature_producer_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22H0pu2s8PkX",
   "metadata": {
    "id": "22H0pu2s8PkX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_one_epoch3(feature_producer_model: nn.Module, optim: torch.optim.Optimizer,\n",
    "         dataloader: DataLoader, loss_fn):\n",
    "\n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    feature_producer_model.train() #\n",
    "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
    "        if digit_labels.shape[0] != 128:\n",
    "            continue\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(images, features, test_size=0.20, random_state=42)\n",
    "\n",
    "        outputs = feature_producer_model(X_train) # Forward Pass, [N, 10]\n",
    "        loss = loss_fn(outputs, y_train) # Compute Loss\n",
    "\n",
    "        loss.backward() # Compute Gradients\n",
    "        optim.step() # Update parameters\n",
    "        optim.zero_grad() # zero the parameter's gradients\n",
    "\n",
    "        running_corrects += 1 - torch.linalg.norm(outputs-y_train) / 256\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_acc = (running_corrects / num_samples) * 100\n",
    "    epoch_loss = (running_loss / num_batches)\n",
    "\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lvtY2t0n87vw",
   "metadata": {
    "id": "lvtY2t0n87vw"
   },
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "kLthH4_88Pif",
   "metadata": {
    "id": "kLthH4_88Pif"
   },
   "outputs": [],
   "source": [
    "def test_model3(feature_producer_model: nn.Module,\n",
    "         dataloader: DataLoader, loss_fn):\n",
    "\n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    feature_producer_model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "    with torch.no_grad():\n",
    "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
    "            if digit_labels.shape[0] != 128:\n",
    "                continue\n",
    "            images = images.to(device)\n",
    "            features = features.to(device)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(images, features, test_size=0.20, random_state=42)\n",
    "            outputs = feature_producer_model(X_test) # Forward Pass\n",
    "            loss = loss_fn(outputs, y_test) # Compute Loss\n",
    "\n",
    "            running_corrects += 1 - torch.linalg.norm(outputs-y_test) / 256\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    test_acc = (running_corrects / num_samples) * 100\n",
    "    test_loss = (running_loss / num_batches)\n",
    "\n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0EwYGbpt8Pba",
   "metadata": {
    "id": "0EwYGbpt8Pba"
   },
   "outputs": [],
   "source": [
    "def demo3():\n",
    "    batch_size = 128\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.005\n",
    "\n",
    "    model = feature_producer_model()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    acc_history = {'train': [], 'test': []}\n",
    "    loss_history = {'train': [], 'test': []}\n",
    "\n",
    "    for epoch in trange(num_epochs):\n",
    "        train_acc, train_loss = train_one_epoch3(feature_producer_model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
    "        test_acc, test_loss = test_model3(feature_producer_model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
    "\n",
    "        acc_history['train'].append(train_acc)\n",
    "        acc_history['test'].append(test_acc)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        loss_history['test'].append(test_loss)\n",
    "    torch.save(model, '/content/model_scripted.pt')\n",
    "\n",
    "    # custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
    "    # print(acc_history)\n",
    "    # print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "_UexJGoT8Ou2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UexJGoT8Ou2",
    "outputId": "a5c6ac34-bd0c-4246-e7f5-4439c09df62d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:55<00:00,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'test': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "demo3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "LnEXwhrjExYJ",
   "metadata": {
    "id": "LnEXwhrjExYJ"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch4(model: nn.Module, optim: torch.optim.Optimizer,\n",
    "         dataloader: DataLoader, loss_fn):\n",
    "\n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train() #\n",
    "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
    "        if digit_labels.shape[0] != 128:\n",
    "            continue\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        digit_labels = digit_labels.to(device)\n",
    "\n",
    "        outputs = model(images, features) # Forward Pass, [N, 10]\n",
    "        loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
    "\n",
    "        loss.backward() # Compute Gradients\n",
    "        optim.step() # Update parameters\n",
    "        optim.zero_grad() # zero the parameter's gradients\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1) # Explain, [N]\n",
    "        running_corrects += torch.sum(preds == digit_labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_acc = (running_corrects / num_samples) * 100\n",
    "    epoch_loss = (running_loss / num_batches)\n",
    "\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8w6Er_SFCnB",
   "metadata": {
    "id": "e8w6Er_SFCnB"
   },
   "outputs": [],
   "source": [
    "def test_model4(model: nn.Module, model_features: nn.Module,\n",
    "         dataloader: DataLoader, loss_fn):\n",
    "\n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "    with torch.no_grad():\n",
    "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
    "            if digit_labels.shape[0] != 128:\n",
    "                continue\n",
    "            images = images.to(device)\n",
    "            features = features.to(device)\n",
    "            features = model_features(images)\n",
    "            digit_labels = digit_labels.to(device)\n",
    "\n",
    "            outputs = model(images, features) # Forward Pass\n",
    "            loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
    "\n",
    "            _, preds = torch.max(outputs, 1) #\n",
    "            running_corrects += torch.sum(preds == digit_labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    test_acc = (running_corrects / num_samples) * 100\n",
    "    test_loss = (running_loss / num_batches)\n",
    "\n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "S0IAj3o8FClJ",
   "metadata": {
    "id": "S0IAj3o8FClJ"
   },
   "outputs": [],
   "source": [
    "def demo4():\n",
    "    batch_size = 128\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.005\n",
    "\n",
    "    model = demo_model()\n",
    "    model = model.to(device)\n",
    "    my_model = torch.load('/content/model_scripted.pt')\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    acc_history = {'train': [], 'test': []}\n",
    "    loss_history = {'train': [], 'test': []}\n",
    "\n",
    "    for epoch in trange(num_epochs):\n",
    "        train_acc, train_loss = train_one_epoch4(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
    "        test_acc, test_loss = test_model4(model=model, model_features=my_model, dataloader=full_dataloaders['test_missing'], loss_fn=cross_entropy)\n",
    "\n",
    "        acc_history['train'].append(train_acc)\n",
    "        acc_history['test'].append(test_acc)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        loss_history['test'].append(test_loss)\n",
    "\n",
    "    # custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
    "    print(acc_history)\n",
    "    # print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "P6GCqfcPFQXy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6GCqfcPFQXy",
    "outputId": "91ed3c9a-fda9-4fce-ba17-b09ed59b1ed4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:22<00:00, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [tensor(21.10, device='cuda:0'), tensor(27.78, device='cuda:0'), tensor(37.38, device='cuda:0'), tensor(45.21, device='cuda:0'), tensor(49.49, device='cuda:0'), tensor(52.48, device='cuda:0'), tensor(55.87, device='cuda:0'), tensor(59.71, device='cuda:0'), tensor(63.88, device='cuda:0'), tensor(68.40, device='cuda:0')], 'test': [tensor(10.56, device='cuda:0'), tensor(10.54, device='cuda:0'), tensor(10.56, device='cuda:0'), tensor(10.52, device='cuda:0'), tensor(10.56, device='cuda:0'), tensor(10.55, device='cuda:0'), tensor(10.55, device='cuda:0'), tensor(10.56, device='cuda:0'), tensor(10.56, device='cuda:0'), tensor(10.54, device='cuda:0')]}\n",
      "{'train': [2.258704131600191, 2.137253085433293, 1.9428035961285328, 1.7148860970031479, 1.5386524350404231, 1.425506397859374, 1.3250914667206786, 1.2251551150004747, 1.1193377108716254, 0.9908559388443351], 'test': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "demo4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "jrBRfmfZciHX",
   "metadata": {
    "id": "jrBRfmfZciHX"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch5(model: nn.Module, optim: torch.optim.Optimizer,\n",
    "         dataloader: DataLoader, loss_fn):\n",
    "\n",
    "    # utils\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train() #\n",
    "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(dataloader): # Get a batch of Data\n",
    "        if digit_labels.shape[0] != 128:\n",
    "            continue\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        digit_labels = digit_labels.to(device)\n",
    "\n",
    "        outputs = model(images, features) # Forward Pass, [N, 10]\n",
    "        loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
    "\n",
    "        loss.backward() # Compute Gradients\n",
    "        optim.step() # Update parameters\n",
    "        optim.zero_grad() # zero the parameter's gradients\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1) # Explain, [N]\n",
    "        running_corrects += torch.sum(preds == digit_labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        features = torch.zeros_like(features).to(device)\n",
    "        outputs = model(images, features) # Forward Pass, [N, 10]\n",
    "        loss = loss_fn(outputs, digit_labels) # Compute Loss\n",
    "\n",
    "        loss.backward() # Compute Gradients\n",
    "        optim.step() # Update parameters\n",
    "        optim.zero_grad() # zero the parameter's gradients\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1) # Explain, [N]\n",
    "        running_corrects += torch.sum(preds == digit_labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_acc = (running_corrects / num_samples) * 100 / 2\n",
    "    epoch_loss = (running_loss / num_batches) / 2\n",
    "\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aU5yWv0bch3m",
   "metadata": {
    "id": "aU5yWv0bch3m"
   },
   "outputs": [],
   "source": [
    "def demo5():\n",
    "    batch_size = 128\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.005\n",
    "\n",
    "    model = demo_model()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    acc_history = {'train': [], 'test': [], 'test_missing': []}\n",
    "    loss_history = {'train': [], 'test': [], 'test_missing': []}\n",
    "\n",
    "    for epoch in trange(num_epochs):\n",
    "        train_acc, train_loss = train_one_epoch5(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
    "        test_acc, test_loss = test_model(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
    "        test_acc_missing, test_loss_missing = test_model(model=model, dataloader=full_dataloaders['test_missing'], loss_fn=cross_entropy)\n",
    "\n",
    "        acc_history['train'].append(train_acc)\n",
    "        acc_history['test'].append(test_acc)\n",
    "        acc_history['test_missing'].append(test_acc_missing)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        loss_history['test'].append(test_loss)\n",
    "        loss_history['test_missing'].append(test_loss_missing)\n",
    "\n",
    "    # custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
    "    print(acc_history)\n",
    "    print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "nZsuhozvZW8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZsuhozvZW8a",
    "outputId": "9c46c485-463f-4e7e-fd9d-f825ca306ecf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:10<00:00, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [tensor(17.92, device='cuda:0'), tensor(27.39, device='cuda:0'), tensor(55.79, device='cuda:0'), tensor(67.74, device='cuda:0'), tensor(74.40, device='cuda:0'), tensor(78.33, device='cuda:0'), tensor(80.87, device='cuda:0'), tensor(82.78, device='cuda:0'), tensor(84.47, device='cuda:0'), tensor(85.87, device='cuda:0')], 'test': [tensor(24.35, device='cuda:0'), tensor(47.45, device='cuda:0'), tensor(62.35, device='cuda:0'), tensor(68.13, device='cuda:0'), tensor(74.62, device='cuda:0'), tensor(76.68, device='cuda:0'), tensor(78.32, device='cuda:0'), tensor(79.39, device='cuda:0'), tensor(80.99, device='cuda:0'), tensor(81.25, device='cuda:0')], 'test_missing': [tensor(13.12, device='cuda:0'), tensor(45.37, device='cuda:0'), tensor(62.05, device='cuda:0'), tensor(67.83, device='cuda:0'), tensor(74.40, device='cuda:0'), tensor(76.56, device='cuda:0'), tensor(78.35, device='cuda:0'), tensor(79.42, device='cuda:0'), tensor(81.04, device='cuda:0'), tensor(81.16, device='cuda:0')]}\n",
      "{'train': [2.27430985909281, 2.069916459670199, 1.3238405616425757, 1.002261058481009, 0.8118548256311335, 0.7006668535504006, 0.6220669063932097, 0.5589956503782445, 0.5082687224978323, 0.4643432500202264], 'test': [2.1937690895690016, 1.5521145565269967, 1.1637360291368157, 0.9990130363131416, 0.8231522088220133, 0.7554248031779859, 0.7089116508086052, 0.6695662726311994, 0.6301696570314599, 0.6274085995360944], 'test_missing': [2.2783816439160227, 1.619393138490485, 1.178915987353353, 1.0111043520227692, 0.8306286786434919, 0.7621270728534495, 0.7142263054847717, 0.6735143661499023, 0.6357764240199998, 0.6323265107073022]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "demo5()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
